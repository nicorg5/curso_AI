{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73df776e-79d0-410f-a8df-783319b1df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from math import ceil,floor\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3646191-16de-4cea-9133-4f3fd1234791",
   "metadata": {},
   "source": [
    "** La clase Agente**\n",
    "\n",
    "Dentro de la clase Agente encontraremos la tabla donde iremos almacenando las políticas. En nuestro caso la tabla cuenta de 3 coordenadas:\n",
    "\n",
    "1. La posición actual del jugador.\n",
    "2. La posición “y” de la pelota.\n",
    "3. La posición en el eje “x” de la pelota.\n",
    "4. \n",
    "Además en esta clase, definiremos el factor de descuento, el learning rate y el ratio de exploración.\n",
    "\n",
    "Los métodos más importantes:\n",
    "\n",
    "- get_next_step() decide la siguiente acción a tomar en base al ratio de exploración si tomar “el mejor paso” que tuviéramos almacenado ó tomar un paso al azar, dando posibilidad a explorar el ambiente\n",
    "- update() aquí se actualizan las políticas mediante la ecuación de Bellman que vimos anteriormente. Es su implementación en python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451a68e5-fb23-4195-a058-7836209cd152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongAgent:\n",
    "    \n",
    "    def __init__(self, game, policy=None, discount_factor = 0.1, learning_rate = 0.1, ratio_explotacion = 0.9):\n",
    " \n",
    "        # Creamos la tabla de politicas\n",
    "        if policy is not None:\n",
    "            self._q_table = policy\n",
    "        else:\n",
    "            position = list(game.positions_space.shape)\n",
    "            position.append(len(game.action_space))\n",
    "            self._q_table = np.zeros(position)\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ratio_explotacion = ratio_explotacion\n",
    " \n",
    "    def get_next_step(self, state, game):\n",
    "        \n",
    "        # Damos un paso aleatorio...\n",
    "        next_step = np.random.choice(list(game.action_space))\n",
    "        \n",
    "        # o tomaremos el mejor paso...\n",
    "        if np.random.uniform() <= self.ratio_explotacion:\n",
    "            # tomar el maximo\n",
    "            idx_action = np.random.choice(np.flatnonzero(\n",
    "                    self._q_table[state[0],state[1],state[2]] == self._q_table[state[0],state[1],state[2]].max()\n",
    "                ))\n",
    "            next_step = list(game.action_space)[idx_action]\n",
    " \n",
    "        return next_step\n",
    " \n",
    "    # actualizamos las politicas con las recompensas obtenidas\n",
    "    def update(self, game, old_state, action_taken, reward_action_taken, new_state, reached_end):\n",
    "        idx_action_taken =list(game.action_space).index(action_taken)\n",
    " \n",
    "        actual_q_value_options = self._q_table[old_state[0], old_state[1], old_state[2]]\n",
    "        actual_q_value = actual_q_value_options[idx_action_taken]\n",
    " \n",
    "        future_q_value_options = self._q_table[new_state[0], new_state[1], new_state[2]]\n",
    "        future_max_q_value = reward_action_taken  +  self.discount_factor*future_q_value_options.max()\n",
    "        if reached_end:\n",
    "            future_max_q_value = reward_action_taken #maximum reward\n",
    " \n",
    "        self._q_table[old_state[0], old_state[1], old_state[2], idx_action_taken] = actual_q_value + \\\n",
    "                                              self.learning_rate*(future_max_q_value -actual_q_value)\n",
    "    \n",
    "    def print_policy(self):\n",
    "        for row in np.round(self._q_table,1):\n",
    "            for column in row:\n",
    "                print('[', end='')\n",
    "                for value in column:\n",
    "                    print(str(value).zfill(5), end=' ')\n",
    "                print('] ', end='')\n",
    "            print('')\n",
    "            \n",
    "    def get_policy(self):\n",
    "        return self._q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a2b2f-908d-49b9-b494-d4183bf79c07",
   "metadata": {},
   "source": [
    "**La clase Environment**\n",
    "\n",
    "En la clase de Ambiente encontramos implementada la lógica y control del juego del pong. Se controla que la pelotita rebote, que no se salga de la pantalla y se encuentran los métodos para graficar y animar en matplotlib.\n",
    "\n",
    "Por Defecto se define una pantalla de 40 pixeles x 50px de alto y si utilizamos la variable “movimiento_px = 5” nos quedará definida nuestra tabla de políticas en 8 de alto y 10 de ancho (por hacer 40/5=8 y 50/5=10). Estos valores se pueden modificar a gusto!\n",
    "\n",
    "Además, muy importante, tenemos el control de cuándo dar las recompensas y penalizaciones, al perder cada vida y detectar si el juego a terminado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c293d90-7fb7-420a-b324-cacde82c6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongEnvironment:\n",
    "    \n",
    "    def __init__(self, max_life=3, height_px = 40, width_px = 50, movimiento_px = 3):\n",
    "        \n",
    "        self.action_space = ['Arriba','Abajo']\n",
    "        \n",
    "        self._step_penalization = 0\n",
    "        \n",
    "        self.state = [0,0,0]\n",
    "        \n",
    "        self.total_reward = 0\n",
    "        \n",
    "        self.dx = movimiento_px\n",
    "        self.dy = movimiento_px\n",
    "        \n",
    "        filas = ceil(height_px/movimiento_px)\n",
    "        columnas = ceil(width_px/movimiento_px)\n",
    "        \n",
    "        self.positions_space = np.array([[[0 for z in range(columnas)] \n",
    "                                                  for y in range(filas)] \n",
    "                                                     for x in range(filas)])\n",
    " \n",
    "        self.lives = max_life\n",
    "        self.max_life=max_life\n",
    "        \n",
    "        self.x = randint(int(width_px/2), width_px) \n",
    "        self.y = randint(0, height_px-10)\n",
    "        \n",
    "        self.player_alto = int(height_px/4)\n",
    " \n",
    "        self.player1 = self.player_alto  # posic. inicial del player\n",
    "        \n",
    "        self.score = 0\n",
    "        \n",
    "        self.width_px = width_px\n",
    "        self.height_px = height_px\n",
    "        self.radio = 2.5\n",
    " \n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "        self.state = [0,0,0]\n",
    "        self.lives = self.max_life\n",
    "        self.score = 0\n",
    "        self.x = randint(int(self.width_px/2), self.width_px) \n",
    "        self.y = randint(0, self.height_px-10)\n",
    "        return self.state\n",
    " \n",
    "    def step(self, action, animate=False):\n",
    "        self._apply_action(action, animate)\n",
    "        done = self.lives <=0 # final\n",
    "        reward = self.score\n",
    "        reward += self._step_penalization\n",
    "        self.total_reward += reward\n",
    "        return self.state, reward , done\n",
    " \n",
    "    def _apply_action(self, action, animate=False):\n",
    "        \n",
    "        if action == \"Arriba\":\n",
    "            self.player1 += abs(self.dy)\n",
    "        elif action == \"Abajo\":\n",
    "            self.player1 -= abs(self.dy)\n",
    "            \n",
    "        self.avanza_player()\n",
    " \n",
    "        self.avanza_frame()\n",
    " \n",
    "        if animate:\n",
    "            clear_output(wait=True);\n",
    "            fig = self.dibujar_frame()\n",
    "            plt.show()\n",
    " \n",
    "        self.state = (floor(self.player1/abs(self.dy))-2, floor(self.y/abs(self.dy))-2, floor(self.x/abs(self.dx))-2)\n",
    "    \n",
    "    def detectaColision(self, ball_y, player_y):\n",
    "        if (player_y+self.player_alto >= (ball_y-self.radio)) and (player_y <= (ball_y+self.radio)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def avanza_player(self):\n",
    "        if self.player1 + self.player_alto >= self.height_px:\n",
    "            self.player1 = self.height_px - self.player_alto\n",
    "        elif self.player1 <= -abs(self.dy):\n",
    "            self.player1 = -abs(self.dy)\n",
    " \n",
    "    def avanza_frame(self):\n",
    "        self.x += self.dx\n",
    "        self.y += self.dy\n",
    "        if self.x <= 3 or self.x > self.width_px:\n",
    "            self.dx = -self.dx\n",
    "            if self.x <= 3:\n",
    "                ret = self.detectaColision(self.y, self.player1)\n",
    " \n",
    "                if ret:\n",
    "                    self.score = 10\n",
    "                else:\n",
    "                    self.score = -10\n",
    "                    self.lives -= 1\n",
    "                    if self.lives>0:\n",
    "                        self.x = randint(int(self.width_px/2), self.width_px)\n",
    "                        self.y = randint(0, self.height_px-10)\n",
    "                        self.dx = abs(self.dx)\n",
    "                        self.dy = abs(self.dy)\n",
    "        else:\n",
    "            self.score = 0\n",
    " \n",
    "        if self.y < 0 or self.y > self.height_px:\n",
    "            self.dy = -self.dy\n",
    " \n",
    "    def dibujar_frame(self):\n",
    "        fig = plt.figure(figsize=(5, 4))\n",
    "        a1 = plt.gca()\n",
    "        circle = plt.Circle((self.x, self.y), self.radio, fc='slategray', ec=\"black\")\n",
    "        a1.set_ylim(-5, self.height_px+5)\n",
    "        a1.set_xlim(-5, self.width_px+5)\n",
    " \n",
    "        rectangle = plt.Rectangle((-5, self.player1), 5, self.player_alto, fc='gold', ec=\"none\")\n",
    "        a1.add_patch(circle);\n",
    "        a1.add_patch(rectangle)\n",
    "        #a1.set_yticklabels([]);a1.set_xticklabels([]);\n",
    "        plt.text(4, self.height_px, \"SCORE:\"+str(self.total_reward)+\"  LIFE:\"+str(self.lives), fontsize=12)\n",
    "        if self.lives <=0:\n",
    "            plt.text(10, self.height_px-14, \"GAME OVER\", fontsize=16)\n",
    "        elif self.total_reward >= 1000:\n",
    "            plt.text(10, self.height_px-14, \"YOU WIN!\", fontsize=16)\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36840634-a830-4526-ba1a-a75ed1c4310e",
   "metadata": {},
   "source": [
    "**El juego: Simular miles de veces para enseñar**\n",
    "\n",
    "Finalmente definimos una función para jugar, donde indicamos la cantidad de veces que queremos iterar la simulación del juego e iremos almacenando algunas estadísticas sobre el comportamiento del agente, si mejora el puntaje con las iteraciones y el máximo puntaje alcanzado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d851d8-e577-47b7-8e3b-742379901d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(rounds=5000, max_life=3, discount_factor = 0.1, learning_rate = 0.1,\n",
    "         ratio_explotacion=0.9,learner=None, game=None, animate=False):\n",
    " \n",
    "    if game is None:\n",
    "        game = PongEnvironment(max_life=max_life, movimiento_px = 3)\n",
    "        \n",
    "    if learner is None:\n",
    "        print(\"Begin new Train!\")\n",
    "        learner = PongAgent(game, discount_factor = discount_factor,learning_rate = learning_rate, ratio_explotacion= ratio_explotacion)\n",
    " \n",
    "    max_points= -9999\n",
    "    first_max_reached = 0\n",
    "    total_rw=0\n",
    "    steps=[]\n",
    " \n",
    "    for played_games in range(0, rounds):\n",
    "        state = game.reset()\n",
    "        reward, done = None, None\n",
    "        \n",
    "        itera=0\n",
    "        while (done != True) and (itera < 3000 and game.total_reward<=1000):\n",
    "            old_state = np.array(state)\n",
    "            next_action = learner.get_next_step(state, game)\n",
    "            state, reward, done = game.step(next_action, animate=animate)\n",
    "            if rounds > 1:\n",
    "                learner.update(game, old_state, next_action, reward, state, done)\n",
    "            itera+=1\n",
    "        \n",
    "        steps.append(itera)\n",
    "        \n",
    "        total_rw+=game.total_reward\n",
    "        if game.total_reward > max_points:\n",
    "            max_points=game.total_reward\n",
    "            first_max_reached = played_games\n",
    "        \n",
    "        if played_games %500==0 and played_games >1 and not animate:\n",
    "            print(\"-- Partidas[\", played_games, \"] Avg.Puntos[\", int(total_rw/played_games),\"]  AVG Steps[\", int(np.array(steps).mean()), \"] Max Score[\", max_points,\"]\")\n",
    "                \n",
    "    if played_games>1:\n",
    "        print('Partidas[',played_games,'] Avg.Puntos[',int(total_rw/played_games),'] Max score[', max_points,'] en partida[',first_max_reached,']')\n",
    "        \n",
    "    #learner.print_policy()\n",
    "    \n",
    "    return learner, game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700ecf7-ac2f-487a-ac28-8eab311c0658",
   "metadata": {},
   "source": [
    "Para entrenar ejecutamos la función con los siguientes parámetros:\n",
    "\n",
    "- 6000 partidas jugará\n",
    "- ratio de explotación: el 85% de las veces será avaro, pero el 15% elige acciones aleatorias, dando lugar a la exploración.\n",
    "- learning rate = se suele dejar en el 10 por ciento como un valor razonable, dando lugar a las recompensas y permitiendo actualizar la importancia de cada acción poco a poco. Tras más iteraciones, mayor importancia tendrá esa acción.\n",
    "- discount_factor = También se suele empezar con valor de 0.1 pero aquí utilizamos un valor del 0.2 para intentar indicar al algoritmo que nos interesa las recompensas a más largo plazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06fedd3c-c71c-48b5-9d29-12b78f5f2248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin new Train!\n",
      "-- Partidas[ 500 ] Avg.Puntos[ 14 ]  AVG Steps[ 219 ] Max Score[ 140 ]\n",
      "-- Partidas[ 1000 ] Avg.Puntos[ 21 ]  AVG Steps[ 244 ] Max Score[ 150 ]\n",
      "-- Partidas[ 1500 ] Avg.Puntos[ 22 ]  AVG Steps[ 245 ] Max Score[ 150 ]\n",
      "-- Partidas[ 2000 ] Avg.Puntos[ 23 ]  AVG Steps[ 248 ] Max Score[ 160 ]\n",
      "-- Partidas[ 2500 ] Avg.Puntos[ 25 ]  AVG Steps[ 256 ] Max Score[ 290 ]\n",
      "-- Partidas[ 3000 ] Avg.Puntos[ 28 ]  AVG Steps[ 267 ] Max Score[ 350 ]\n",
      "-- Partidas[ 3500 ] Avg.Puntos[ 30 ]  AVG Steps[ 274 ] Max Score[ 350 ]\n",
      "-- Partidas[ 4000 ] Avg.Puntos[ 32 ]  AVG Steps[ 279 ] Max Score[ 350 ]\n",
      "-- Partidas[ 4500 ] Avg.Puntos[ 32 ]  AVG Steps[ 279 ] Max Score[ 350 ]\n",
      "-- Partidas[ 5000 ] Avg.Puntos[ 33 ]  AVG Steps[ 282 ] Max Score[ 350 ]\n",
      "-- Partidas[ 5500 ] Avg.Puntos[ 33 ]  AVG Steps[ 284 ] Max Score[ 390 ]\n",
      "Partidas[ 5999 ] Avg.Puntos[ 34 ] Max score[ 390 ] en partida[ 5036 ]\n"
     ]
    }
   ],
   "source": [
    "learner, game = play(rounds=6000, discount_factor = 0.2, learning_rate = 0.1, ratio_explotacion=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfa293-edb4-4c80-a91b-7e5c175e31bd",
   "metadata": {},
   "source": [
    "En las salidas vemos sobre todo cómo va mejorando en la cantidad de “steps” que da el agente antes de perder la partida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dd96b-6d58-4b3f-a78b-661959714cd2",
   "metadata": {},
   "source": [
    "**Veamos el resultado!**\n",
    "\n",
    "Ya contamos con nuestro agente entrenado, ahora veamos qué tal se comporta en una partida de pong, y lo podemos ver jugar, pasando el parámetro animate=True.\n",
    "\n",
    "Antes de jugar, instanciamos un nuevo agente “learner2” que utilizará las políticas que creamos anteriormente. A este agente le seteamos el valor de explotación en 1, para evitar que tome pasos aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6eb927a-2815-4f97-a1f3-03a041558728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner2 = PongAgent(game, policy=learner.get_policy())\n",
    "learner2.ratio_explotacion = 1.0  # con esto quitamos las elecciones aleatorias al jugar\n",
    "player = play(rounds=1, learner=learner2, game=game, animate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
