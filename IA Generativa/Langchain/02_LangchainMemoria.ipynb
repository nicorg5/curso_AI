{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dca9be",
   "metadata": {
    "id": "46dca9be"
   },
   "source": [
    "## Memoria - ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59186ead",
   "metadata": {
    "id": "59186ead"
   },
   "outputs": [],
   "source": [
    "# Primero importamos las bibliotecas necesarias:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4279b7-7088-484f-87dc-35e6f0dbe99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "# Configurar el motor de OpenAI\n",
    "engine = \"gpt-3.5-turbo\"\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fc4e39",
   "metadata": {
    "id": "31fc4e39"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=engine, openai_api_key=api_key)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=False # Si ponemos verbose=True devuelve datos de la plantilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dead56ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dead56ac",
    "outputId": "46cda413-01aa-42cd-bf9d-a6101f1d5602"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola Carlos! ¿Cómo puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hola, mi nombre es Carlos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca30bdd6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ca30bdd6",
    "outputId": "cd4b67d4-1a1d-467f-85ea-3ebdd19d62db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La capital de Honduras es Tegucigalpa.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Cuál es la capital de Honduras?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17bf05d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "17bf05d7",
    "outputId": "1cfe3587-97c2-46cb-b6a6-c96d568afced"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tu nombre es Carlos.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Cuál es mi nombre?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc758e3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc758e3e",
    "outputId": "21e624f8-a638-4355-b781-c4e5b7c7e0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hola, mi nombre es Carlos\n",
      "AI: ¡Hola Carlos! ¿Cómo puedo ayudarte hoy?\n",
      "Human: ¿Cuál es la capital de Honduras?\n",
      "AI: La capital de Honduras es Tegucigalpa.\n",
      "Human: ¿Cuál es mi nombre?\n",
      "AI: Tu nombre es Carlos.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcaaad1",
   "metadata": {
    "id": "8bcaaad1"
   },
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26079aed",
   "metadata": {
    "id": "26079aed"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=engine, openai_api_key=api_key)\n",
    "## USamos conversationBufferWindowMemory para almacenar el contexto de la conversación\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1) # Numero de interacciones en AI y usuario\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=False # Si usas True podrás ver el prompt real que se está pasando al modelo\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e28810ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e28810ec",
    "outputId": "56fcea2e-4d9d-4db9-8a11-96025c8d5e92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola Carlos! ¿Cómo puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hola, mi nombre es Carlos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f90284a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "f90284a1",
    "outputId": "6b7b622e-317c-4c44-e4d4-6ac91d765621"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La capital de Honduras es Tegucigalpa.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Cuál es la capital de Honduras?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc4ba82d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fc4ba82d",
    "outputId": "3f9b2d10-cedc-4a7b-dfd1-169bcc600242"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lo siento, no tengo acceso a tu información personal, por lo que no sé cuál es tu nombre.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"¿Cuál es mi nombre\") # No lo recordará ya que k=1. Cambia k=2 para ver que funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b94a6609",
   "metadata": {
    "id": "b94a6609"
   },
   "outputs": [],
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8501edb",
   "metadata": {
    "id": "d8501edb"
   },
   "outputs": [],
   "source": [
    "# creamos un texto\n",
    "schedule = \"Hay una reunión a las 8 a. m. con tu equipo de producto. \\\n",
    "Necesitarás tener preparada tu presentación de PowerPoint. \\\n",
    "De 9 a. m. a 12 p. m. tienes tiempo para trabajar en tu proyecto LangChain, \\\n",
    "que avanzará rápidamente porque LangChain es una herramienta muy potente.\\\n",
    "Al mediodía, almuerzo en el restaurante italiano con un cliente que viene conduciendo\\\n",
    "desde más de una hora de distancia para reunirse contigo y entender lo último en IA.\\\n",
    "Asegúrate de traer tu portátil para mostrar la última demo de LLM.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hola\"}, {\"output\": \"¿Qué hay?\"})\n",
    "memory.save_context({\"input\": \"No mucho, por aquí...\"},\n",
    "                    {\"output\": \"Guay\"})\n",
    "memory.save_context({\"input\": \"Qué hay en la agenda hoy?\"},\n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5416e057",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5416e057",
    "outputId": "6c2e4b8c-78d4-4037-80cf-392528a1cd14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI engage in a conversation about their daily agenda. The AI informs the human about a morning meeting with their product team and the need to prepare a PowerPoint presentation. The AI also mentions a project called LangChain that will progress quickly due to its powerful capabilities. The AI then mentions a lunch meeting with a client who has traveled a long distance to discuss the latest in artificial intelligence. The AI advises the human to bring their laptop to showcase the latest demo of LLM.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cdf5bd2",
   "metadata": {
    "id": "7cdf5bd2"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "056a7ab4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "056a7ab4",
    "outputId": "b02b647f-8bbd-479e-f544-1e0bb7c69d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI engage in a conversation about their daily agenda. The AI informs the human about a morning meeting with their product team and the need to prepare a PowerPoint presentation. The AI also mentions a project called LangChain that will progress quickly due to its powerful capabilities. The AI then mentions a lunch meeting with a client who has traveled a long distance to discuss the latest in artificial intelligence. The AI advises the human to bring their laptop to showcase the latest demo of LLM.\n",
      "Human: Qué hay en la agenda para hoy?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hola! En la agenda de hoy tenemos una reunión por la mañana con el equipo de productos, donde necesitaremos preparar una presentación en PowerPoint. También tenemos un proyecto llamado LangChain que está avanzando rápidamente debido a sus poderosas capacidades. Después, tenemos una reunión de almuerzo con un cliente que ha viajado desde lejos para discutir las últimas novedades en inteligencia artificial. Te aconsejo que traigas tu laptop para mostrar la última demostración de LLM. ¿Hay algo más en lo que pueda ayudarte?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Qué hay en la agenda para hoy?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
